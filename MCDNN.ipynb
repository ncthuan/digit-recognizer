{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "u57IDPdT9ES7",
    "outputId": "681cbe15-3491-4aab-a691-48ae4cbd1daa"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "!ls \"/content/gdrive/My Drive/lab/MNIST/\"\n",
    "dir = \"/content/gdrive/My Drive/lab/MNIST/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "uu1uGPoiRUyf",
    "outputId": "c9d4ed84-a9df-4391-ed77-9282df581508"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HIxCBgFD85_O",
    "outputId": "ccc6a8bf-a234-48e5-e22a-9250ab6cc178"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e8xXS3e085_e"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not running on colab, execute this cell\n",
    "dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JCj7Jora85_f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 28, 28, 1) (42000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Train data: 42000 images\n",
    "train_data = pd.read_csv(dir+\"dataset/train.csv\")\n",
    "train_data = train_data.to_numpy(dtype='float32')\n",
    "x, y = train_data[:,1:], train_data[:,0]\n",
    "x = x.reshape(x.shape[0], 28, 28, 1)\n",
    "# Convert labels to one hot encoding\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(y)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Z4hbk7s85_o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 28, 28, 1) (28000,)\n"
     ]
    }
   ],
   "source": [
    "# Test data: 28000 images\n",
    "test_data = pd.read_csv(dir+\"dataset/testset_with_label.csv\")\n",
    "x_test = test_data.iloc[:,0:784].to_numpy(dtype='float32').reshape((len(test_data),28,28,1))\n",
    "# get label of the testset\n",
    "y_test = test_data['Label'].to_numpy()\n",
    "\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "ZTlGZBcb85_5",
    "outputId": "c7903b9f-2a4a-495a-998d-29d45a07f8f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21f990ff808>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[1,:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYIHMgsQ86AA"
   },
   "source": [
    "### Normalize digits' width and pixel value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CYse1oLK86AB",
    "outputId": "e7554ff8-ee11-47bd-f263-a85fb0367ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 29, 29, 1)\n",
      "(28000, 29, 29, 1)\n"
     ]
    }
   ],
   "source": [
    "x = tf.pad(x, tf.constant([[0,0], [0,1], [0,1], [0,0]]))\n",
    "x_test = tf.pad(x_test, tf.constant([[0,0], [0,1], [0,1], [0,0]]))\n",
    "print(x.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50eKpP2086Ab"
   },
   "outputs": [],
   "source": [
    "def get_data_with_size(data, normalize_size = 15, target_size = 29):\n",
    "    assert data.shape[1:3] == (29,29)\n",
    "    assert normalize_size % 2 == 1 and target_size % 2 == 1\n",
    "    if normalize_size == target_size:\n",
    "        return data\n",
    "    data = tf.image.resize(data, (target_size, normalize_size))\n",
    "    paddings = (target_size - normalize_size)//2\n",
    "    data = tf.pad(data, tf.constant([[0,0], [0,0], [paddings,paddings], [0,0]]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MRVc4_z86Ag"
   },
   "outputs": [],
   "source": [
    "NORMALIZE_SIZES = [29,27,25,21,17]\n",
    "data_train = dict()\n",
    "data_test = dict()\n",
    "for size in NORMALIZE_SIZES:\n",
    "    data_train[str(size)] = get_data_with_size(x,size).numpy() / 255.0  #convert to numpy data and normalize value to 0-1\n",
    "    data_test[str(size)]  = get_data_with_size(x_test,size).numpy() / 255.0\n",
    "data_train['y'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhbHSxAJ86An"
   },
   "source": [
    "### Build models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbdADV_686Ao"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters = 20, kernel_size = (4,4), padding = 'valid', activation ='tanh', input_shape = (29,29,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 40, kernel_size = (5,5), padding = 'valid', activation ='tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(150, activation ='tanh'))\n",
    "    model.add(Dense(10, activation ='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zD5ahnWy86A0",
    "outputId": "0d064534-3719-4dd2-d0ba-5f829d3d21a5"
   },
   "outputs": [],
   "source": [
    "x = data_train[str(21)].numpy()/255.0 #normalize\n",
    "y = data_train['y']\n",
    "\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size = 0.1)\n",
    "model = build_model()\n",
    "datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1)\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.98 ** x)\n",
    "batch_size = 128\n",
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size), epochs = 100, steps_per_epoch = x_train.shape[0]//batch_size,\n",
    "                    validation_data = (x_dev, y_dev), callbacks=[annealer], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5b7MdewFJiu3"
   },
   "outputs": [],
   "source": [
    "model.save(dir+'/models/first_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FnFDouyO86BD"
   },
   "source": [
    "### Train models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YmmD7hP086BE"
   },
   "outputs": [],
   "source": [
    "models = dict()\n",
    "for normalize_size in NORMALIZE_SIZES[1:]:\n",
    "    for column in range(0,4):\n",
    "        model_name = f'{normalize_size}_{column}'\n",
    "        # prepare data\n",
    "        x = data[str(normalize_size)].numpy()/255.0\n",
    "        y = data['y']\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size = 0.1)\n",
    "\n",
    "        # data augmentation\n",
    "        datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1)\n",
    "\n",
    "        # callbacks\n",
    "        annealer = LearningRateScheduler(lambda x: 1e-3 * 0.98 ** x)\n",
    "        logger = CSVLogger(dir+'/models/training_history/'+model_name+'.csv', append=True)\n",
    "\n",
    "        # define some params\n",
    "        batch_size = 128\n",
    "        epochs = 100\n",
    "\n",
    "        # start training\n",
    "        model = build_model()\n",
    "        model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "        model.fit_generator(datagen.flow(x_train, y_train, batch_size), epochs=epochs, steps_per_epoch = x_train.shape[0]//batch_size,\n",
    "                            validation_data=(x_dev, y_dev), validation_freq=1, callbacks=[annealer, logger], verbose=0)\n",
    "        \n",
    "        models['model_name'] = model\n",
    "        #save model\n",
    "        model.save(dir+'/models/'+model_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLf0x4Di86BK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A5ULnPt--c1g"
   },
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdCCjJ3h-hqL"
   },
   "outputs": [],
   "source": [
    "# load models\n",
    "models = dict()\n",
    "for model_name in os.listdir(dir+'models/'):\n",
    "    model = load_model(dir+'models/'+model_name)\n",
    "    model_id = model_name[:-5]\n",
    "    models[model_id] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2senRQbxFzm"
   },
   "outputs": [],
   "source": [
    "def predict(models, x):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        models: a dict of models that was trained and loaded above\n",
    "        x: a dict of transformed testsets (#n, h, w, channels)\n",
    "    Return:\n",
    "        predictions: a dict of predictions made by individual nets and multi-column nets\n",
    "    \"\"\"\n",
    "    predictions = dict()\n",
    "    all_net_sum = 0\n",
    "\n",
    "    for normalize_size in NORMALIZE_SIZES:\n",
    "        four_net_sum = 0\n",
    "        for column in range(4):\n",
    "            model_id = f'{normalize_size}_{column}'\n",
    "            column_val = models[model_id].predict(x[str(normalize_size)])\n",
    "            predictions[model_id] = np.argmax(column_val, axis=1)\n",
    "\n",
    "            four_net_sum += column_val\n",
    "            all_net_sum += column_val\n",
    "\n",
    "        four_net_avg = four_net_sum / 4 # take 4-column average\n",
    "        predictions[str(normalize_size)] = np.argmax(four_net_avg, axis=1)\n",
    "\n",
    "    all_net_avg = all_net_sum / len(models)\n",
    "    predictions['avg'] = np.argmax(all_net_avg, axis=1) # take average of all models\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3RKAqJIhDVxx"
   },
   "outputs": [],
   "source": [
    "def error_rate(prediction, ground_truth):\n",
    "    assert len(prediction) == len(ground_truth)\n",
    "    a = accuracy_score(prediction, ground_truth)\n",
    "    e = 1-a\n",
    "    return f'{e*100:.2f}%' #string format .2f %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cehfNuVKF171"
   },
   "outputs": [],
   "source": [
    "predictions = predict(models, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "Nu-Y0-FH290a",
    "outputId": "bdd15e4f-b48e-4b45-c2b8-e6df4403ef9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29_0 0.72%\n",
      "29_1 0.69%\n",
      "29_2 0.64%\n",
      "29_3 0.59%\n",
      "29 0.48%\n",
      "27_0 0.59%\n",
      "27_1 0.64%\n",
      "27_2 0.64%\n",
      "27_3 0.65%\n",
      "27 0.45%\n",
      "25_0 0.58%\n",
      "25_1 0.58%\n",
      "25_2 0.73%\n",
      "25_3 0.56%\n",
      "25 0.45%\n",
      "21_0 0.66%\n",
      "21_1 0.61%\n",
      "21_2 0.72%\n",
      "21_3 0.71%\n",
      "21 0.48%\n",
      "17_0 0.71%\n",
      "17_1 0.60%\n",
      "17_2 0.59%\n",
      "17_3 0.70%\n",
      "17 0.48%\n",
      "avg 0.40%\n"
     ]
    }
   ],
   "source": [
    "for model_id, pred in predictions.items():\n",
    "    print(model_id, error_rate(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-UB4S3WGr8u"
   },
   "source": [
    "### Ensemble with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4tI13LUOGyv1"
   },
   "outputs": [],
   "source": [
    "# Create data for training weights for each model\n",
    "def get_model_outputs(models, x):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        models: a dict of models that was trained and loaded above\n",
    "        x: a dict of transformed testsets (#n, h, w, channels)\n",
    "    Return:\n",
    "        outputs: 20 columns get stacked together (#n, 10, 20, 1) and a dict maping model_id to column id\n",
    "    \"\"\"\n",
    "    outputs = np.zeros((x['29'].shape[0],10,20,1), dtype='float32')\n",
    "    model_to_column = dict()\n",
    "    \n",
    "    i = 0\n",
    "    for normalize_size in NORMALIZE_SIZES:\n",
    "        for column in range(4):\n",
    "            model_id = f'{normalize_size}_{column}'\n",
    "            column_val = models[model_id].predict(x[str(normalize_size)])\n",
    "            outputs[:,:,i,0] = column_val\n",
    "            model_to_column[model_id] = i\n",
    "            i += 1\n",
    "            \n",
    "    return (outputs, model_to_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10, 20, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns, model_to_columnId = get_model_outputs(models, data_train)\n",
    "columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensembler(x, y):\n",
    "    \"\"\"\n",
    "    x: outputs of 20 models (42000,10,20,1)\n",
    "    y: label (42000,10)\n",
    "    return: weights for models\n",
    "    \"\"\"\n",
    "    # prepare data\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size = 0.2, random_state=42)\n",
    "    \n",
    "    ensembler = Sequential()\n",
    "    ensembler.add(Conv2D(filters=1, kernel_size=(1,20), padding='valid', input_shape=(10,20,1)))\n",
    "    ensembler.add(Flatten())\n",
    "    ensembler.add(Activation('softmax'))\n",
    "    \n",
    "    annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "    ensembler.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "    ensembler.fit(x=x_train, y=y_train, epochs=42, batch_size=256, validation_data=(x_dev, y_dev), callbacks=[annealer], verbose=2)\n",
    "    \n",
    "    return ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/42\n",
      " - 1s - loss: 1.8673 - accuracy: 0.7166 - val_loss: 0.9511 - val_accuracy: 0.9990\n",
      "Epoch 2/42\n",
      " - 1s - loss: 0.5555 - accuracy: 0.9993 - val_loss: 0.3051 - val_accuracy: 0.9993\n",
      "Epoch 3/42\n",
      " - 0s - loss: 0.2133 - accuracy: 0.9993 - val_loss: 0.1477 - val_accuracy: 0.9995\n",
      "Epoch 4/42\n",
      " - 0s - loss: 0.1163 - accuracy: 0.9994 - val_loss: 0.0905 - val_accuracy: 0.9996\n",
      "Epoch 5/42\n",
      " - 1s - loss: 0.0763 - accuracy: 0.9994 - val_loss: 0.0630 - val_accuracy: 0.9996\n",
      "Epoch 6/42\n",
      " - 0s - loss: 0.0554 - accuracy: 0.9994 - val_loss: 0.0473 - val_accuracy: 0.9995\n",
      "Epoch 7/42\n",
      " - 0s - loss: 0.0429 - accuracy: 0.9994 - val_loss: 0.0374 - val_accuracy: 0.9995\n",
      "Epoch 8/42\n",
      " - 0s - loss: 0.0347 - accuracy: 0.9994 - val_loss: 0.0306 - val_accuracy: 0.9995\n",
      "Epoch 9/42\n",
      " - 1s - loss: 0.0290 - accuracy: 0.9994 - val_loss: 0.0258 - val_accuracy: 0.9995\n",
      "Epoch 10/42\n",
      " - 1s - loss: 0.0249 - accuracy: 0.9994 - val_loss: 0.0222 - val_accuracy: 0.9995\n",
      "Epoch 11/42\n",
      " - 1s - loss: 0.0217 - accuracy: 0.9994 - val_loss: 0.0194 - val_accuracy: 0.9995\n",
      "Epoch 12/42\n",
      " - 0s - loss: 0.0193 - accuracy: 0.9994 - val_loss: 0.0172 - val_accuracy: 0.9995\n",
      "Epoch 13/42\n",
      " - 0s - loss: 0.0173 - accuracy: 0.9994 - val_loss: 0.0154 - val_accuracy: 0.9995\n",
      "Epoch 14/42\n",
      " - 1s - loss: 0.0157 - accuracy: 0.9994 - val_loss: 0.0139 - val_accuracy: 0.9995\n",
      "Epoch 15/42\n",
      " - 1s - loss: 0.0144 - accuracy: 0.9994 - val_loss: 0.0127 - val_accuracy: 0.9995\n",
      "Epoch 16/42\n",
      " - 0s - loss: 0.0133 - accuracy: 0.9994 - val_loss: 0.0117 - val_accuracy: 0.9995\n",
      "Epoch 17/42\n",
      " - 0s - loss: 0.0123 - accuracy: 0.9994 - val_loss: 0.0108 - val_accuracy: 0.9996\n",
      "Epoch 18/42\n",
      " - 0s - loss: 0.0115 - accuracy: 0.9994 - val_loss: 0.0100 - val_accuracy: 0.9996\n",
      "Epoch 19/42\n",
      " - 1s - loss: 0.0108 - accuracy: 0.9994 - val_loss: 0.0093 - val_accuracy: 0.9996\n",
      "Epoch 20/42\n",
      " - 0s - loss: 0.0102 - accuracy: 0.9994 - val_loss: 0.0087 - val_accuracy: 0.9996\n",
      "Epoch 21/42\n",
      " - 1s - loss: 0.0097 - accuracy: 0.9994 - val_loss: 0.0082 - val_accuracy: 0.9996\n",
      "Epoch 22/42\n",
      " - 0s - loss: 0.0092 - accuracy: 0.9994 - val_loss: 0.0077 - val_accuracy: 0.9996\n",
      "Epoch 23/42\n",
      " - 0s - loss: 0.0088 - accuracy: 0.9994 - val_loss: 0.0073 - val_accuracy: 0.9996\n",
      "Epoch 24/42\n",
      " - 0s - loss: 0.0084 - accuracy: 0.9994 - val_loss: 0.0070 - val_accuracy: 0.9996\n",
      "Epoch 25/42\n",
      " - 1s - loss: 0.0081 - accuracy: 0.9994 - val_loss: 0.0066 - val_accuracy: 0.9996\n",
      "Epoch 26/42\n",
      " - 0s - loss: 0.0078 - accuracy: 0.9994 - val_loss: 0.0063 - val_accuracy: 0.9996\n",
      "Epoch 27/42\n",
      " - 0s - loss: 0.0075 - accuracy: 0.9994 - val_loss: 0.0060 - val_accuracy: 0.9996\n",
      "Epoch 28/42\n",
      " - 0s - loss: 0.0072 - accuracy: 0.9994 - val_loss: 0.0058 - val_accuracy: 0.9996\n",
      "Epoch 29/42\n",
      " - 0s - loss: 0.0070 - accuracy: 0.9994 - val_loss: 0.0055 - val_accuracy: 0.9996\n",
      "Epoch 30/42\n",
      " - 0s - loss: 0.0068 - accuracy: 0.9994 - val_loss: 0.0053 - val_accuracy: 0.9996\n",
      "Epoch 31/42\n",
      " - 1s - loss: 0.0066 - accuracy: 0.9994 - val_loss: 0.0051 - val_accuracy: 0.9996\n",
      "Epoch 32/42\n",
      " - 0s - loss: 0.0064 - accuracy: 0.9994 - val_loss: 0.0049 - val_accuracy: 0.9996\n",
      "Epoch 33/42\n",
      " - 0s - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9996\n",
      "Epoch 34/42\n",
      " - 0s - loss: 0.0061 - accuracy: 0.9994 - val_loss: 0.0046 - val_accuracy: 0.9996\n",
      "Epoch 35/42\n",
      " - 0s - loss: 0.0059 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9996\n",
      "Epoch 36/42\n",
      " - 1s - loss: 0.0058 - accuracy: 0.9994 - val_loss: 0.0043 - val_accuracy: 0.9996\n",
      "Epoch 37/42\n",
      " - 0s - loss: 0.0057 - accuracy: 0.9994 - val_loss: 0.0042 - val_accuracy: 0.9996\n",
      "Epoch 38/42\n",
      " - 0s - loss: 0.0056 - accuracy: 0.9994 - val_loss: 0.0041 - val_accuracy: 0.9996\n",
      "Epoch 39/42\n",
      " - 1s - loss: 0.0055 - accuracy: 0.9994 - val_loss: 0.0039 - val_accuracy: 0.9996\n",
      "Epoch 40/42\n",
      " - 0s - loss: 0.0053 - accuracy: 0.9994 - val_loss: 0.0038 - val_accuracy: 0.9996\n",
      "Epoch 41/42\n",
      " - 1s - loss: 0.0053 - accuracy: 0.9994 - val_loss: 0.0037 - val_accuracy: 0.9996\n",
      "Epoch 42/42\n",
      " - 0s - loss: 0.0052 - accuracy: 0.9994 - val_loss: 0.0036 - val_accuracy: 0.9996\n"
     ]
    }
   ],
   "source": [
    "ensembler1 = train_ensembler(columns, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[0.51343805]],\n",
      "\n",
      "        [[0.555836  ]],\n",
      "\n",
      "        [[0.6518161 ]],\n",
      "\n",
      "        [[0.5099554 ]],\n",
      "\n",
      "        [[0.2648691 ]],\n",
      "\n",
      "        [[0.07180303]],\n",
      "\n",
      "        [[0.1249533 ]],\n",
      "\n",
      "        [[0.71428734]],\n",
      "\n",
      "        [[0.03785058]],\n",
      "\n",
      "        [[0.56190246]],\n",
      "\n",
      "        [[0.33768785]],\n",
      "\n",
      "        [[0.69464344]],\n",
      "\n",
      "        [[0.6920143 ]],\n",
      "\n",
      "        [[0.04796052]],\n",
      "\n",
      "        [[0.08143652]],\n",
      "\n",
      "        [[0.2912201 ]],\n",
      "\n",
      "        [[0.32147604]],\n",
      "\n",
      "        [[0.6022201 ]],\n",
      "\n",
      "        [[0.06269673]],\n",
      "\n",
      "        [[0.62326115]]]], dtype=float32), array([2.7938367e-09], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "weights = ensembler1.get_layer(index=0).get_weights()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[0.5282702 ]],\n",
      "\n",
      "        [[0.34110475]],\n",
      "\n",
      "        [[0.79952043]],\n",
      "\n",
      "        [[0.57873416]],\n",
      "\n",
      "        [[0.6989094 ]],\n",
      "\n",
      "        [[0.32333928]],\n",
      "\n",
      "        [[0.5689375 ]],\n",
      "\n",
      "        [[0.13514563]],\n",
      "\n",
      "        [[0.1143741 ]],\n",
      "\n",
      "        [[0.79800946]],\n",
      "\n",
      "        [[0.47129872]],\n",
      "\n",
      "        [[0.2930282 ]],\n",
      "\n",
      "        [[0.4485501 ]],\n",
      "\n",
      "        [[0.33545163]],\n",
      "\n",
      "        [[0.3575827 ]],\n",
      "\n",
      "        [[0.05459918]],\n",
      "\n",
      "        [[0.25592005]],\n",
      "\n",
      "        [[0.19542356]],\n",
      "\n",
      "        [[0.05737453]],\n",
      "\n",
      "        [[0.8009204 ]]]], dtype=float32), array([0.00027843], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "weights = ensembler1.get_layer(index=0).get_weights()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ensembler on testset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_columns, _ = get_model_outputs(models, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ensembler1.predict(testset_columns)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('dataset/test_columns')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MCDNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
