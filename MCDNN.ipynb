{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "u57IDPdT9ES7",
    "outputId": "681cbe15-3491-4aab-a691-48ae4cbd1daa"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "!ls \"/content/gdrive/My Drive/lab/MNIST/\"\n",
    "dir = \"/content/gdrive/My Drive/lab/MNIST/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "uu1uGPoiRUyf",
    "outputId": "c9d4ed84-a9df-4391-ed77-9282df581508"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HIxCBgFD85_O",
    "outputId": "ccc6a8bf-a234-48e5-e22a-9250ab6cc178"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, CSVLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e8xXS3e085_e"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not running on colab, execute this cell\n",
    "dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JCj7Jora85_f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 28, 28, 1) (42000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Train data: 42000 images\n",
    "train_data = pd.read_csv(dir+\"dataset/train.csv\")\n",
    "train_data = train_data.to_numpy(dtype='float32')\n",
    "x, y = train_data[:,1:], train_data[:,0]\n",
    "x = x.reshape(x.shape[0], 28, 28, 1)\n",
    "# Convert labels to one hot encoding\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(y)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Z4hbk7s85_o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 28, 28, 1) (28000,)\n"
     ]
    }
   ],
   "source": [
    "# Test data: 28000 images\n",
    "test_data = pd.read_csv(dir+\"dataset/testset_with_label.csv\")\n",
    "x_test = test_data.iloc[:,0:784].to_numpy(dtype='float32').reshape((len(test_data),28,28,1))\n",
    "# get label of the testset\n",
    "y_test = test_data['Label'].to_numpy()\n",
    "\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "ZTlGZBcb85_5",
    "outputId": "c7903b9f-2a4a-495a-998d-29d45a07f8f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21fd12f08c8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[2,:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYIHMgsQ86AA"
   },
   "source": [
    "### Normalize digits' width and pixel value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CYse1oLK86AB",
    "outputId": "e7554ff8-ee11-47bd-f263-a85fb0367ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 29, 29, 1)\n",
      "(28000, 29, 29, 1)\n"
     ]
    }
   ],
   "source": [
    "x = tf.pad(x, tf.constant([[0,0], [0,1], [0,1], [0,0]]))\n",
    "x_test = tf.pad(x_test, tf.constant([[0,0], [0,1], [0,1], [0,0]]))\n",
    "print(x.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50eKpP2086Ab"
   },
   "outputs": [],
   "source": [
    "def get_data_with_size(data, normalize_size = 15, target_size = 29):\n",
    "    assert data.shape[1:3] == (29,29)\n",
    "    assert normalize_size % 2 == 1 and target_size % 2 == 1\n",
    "    if normalize_size == target_size:\n",
    "        return data\n",
    "    data = tf.image.resize(data, (target_size, normalize_size))\n",
    "    paddings = (target_size - normalize_size)//2\n",
    "    data = tf.pad(data, tf.constant([[0,0], [0,0], [paddings,paddings], [0,0]]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MRVc4_z86Ag"
   },
   "outputs": [],
   "source": [
    "NORMALIZE_SIZES = [29,27,25,21,17]\n",
    "data_train = dict()\n",
    "data_test = dict()\n",
    "for size in NORMALIZE_SIZES:\n",
    "    data_train[str(size)] = get_data_with_size(x,size).numpy() / 255.0  #convert to numpy data and normalize value to 0-1\n",
    "    data_test[str(size)]  = get_data_with_size(x_test,size).numpy() / 255.0\n",
    "data_train['y'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhbHSxAJ86An"
   },
   "source": [
    "### Build models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbdADV_686Ao"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters = 20, kernel_size = (4,4), padding = 'valid', activation ='tanh', input_shape = (29,29,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(filters = 40, kernel_size = (5,5), padding = 'valid', activation ='tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(150, activation ='tanh'))\n",
    "    model.add(Dense(10, activation ='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zD5ahnWy86A0",
    "outputId": "0d064534-3719-4dd2-d0ba-5f829d3d21a5"
   },
   "outputs": [],
   "source": [
    "x = data_train[str(21)].numpy()/255.0 #normalize\n",
    "y = data_train['y']\n",
    "\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size = 0.1)\n",
    "model = build_model()\n",
    "datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1)\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.98 ** x)\n",
    "batch_size = 128\n",
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size), epochs = 100, steps_per_epoch = x_train.shape[0]//batch_size,\n",
    "                    validation_data = (x_dev, y_dev), callbacks=[annealer], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5b7MdewFJiu3"
   },
   "outputs": [],
   "source": [
    "model.save(dir+'/models/first_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FnFDouyO86BD"
   },
   "source": [
    "### Train models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YmmD7hP086BE"
   },
   "outputs": [],
   "source": [
    "models = dict()\n",
    "for normalize_size in NORMALIZE_SIZES[1:]:\n",
    "    for column in range(0,4):\n",
    "        model_name = f'{normalize_size}_{column}'\n",
    "        # prepare data\n",
    "        x = data[str(normalize_size)].numpy()/255.0\n",
    "        y = data['y']\n",
    "        x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size = 0.1)\n",
    "\n",
    "        # data augmentation\n",
    "        datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1)\n",
    "\n",
    "        # callbacks\n",
    "        annealer = LearningRateScheduler(lambda x: 1e-3 * 0.98 ** x)\n",
    "        logger = CSVLogger(dir+'/models/training_history/'+model_name+'.csv', append=True)\n",
    "\n",
    "        # define some params\n",
    "        batch_size = 128\n",
    "        epochs = 100\n",
    "\n",
    "        # start training\n",
    "        model = build_model()\n",
    "        model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "        model.fit_generator(datagen.flow(x_train, y_train, batch_size), epochs=epochs, steps_per_epoch = x_train.shape[0]//batch_size,\n",
    "                            validation_data=(x_dev, y_dev), validation_freq=1, callbacks=[annealer, logger], verbose=0)\n",
    "        \n",
    "        models['model_name'] = model\n",
    "        #save model\n",
    "        model.save(dir+'/models/'+model_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLf0x4Di86BK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A5ULnPt--c1g"
   },
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdCCjJ3h-hqL"
   },
   "outputs": [],
   "source": [
    "# load models\n",
    "models = dict()\n",
    "for model_name in os.listdir(dir+'models/'):\n",
    "    model = load_model(dir+'models/'+model_name)\n",
    "    model_id = model_name[:-5]\n",
    "    models[model_id] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2senRQbxFzm"
   },
   "outputs": [],
   "source": [
    "def predict(models, x):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        models: a dict of models that was trained and loaded above\n",
    "        x: a dict of transformed testsets (#n, h, w, channels)\n",
    "    Return:\n",
    "        predictions: a dict of predictions made by individual nets and multi-column nets\n",
    "    \"\"\"\n",
    "    predictions = dict()\n",
    "    all_net_sum = 0\n",
    "\n",
    "    for normalize_size in NORMALIZE_SIZES:\n",
    "        four_net_sum = 0\n",
    "        for column in range(4):\n",
    "            model_id = f'{normalize_size}_{column}'\n",
    "            column_val = models[model_id].predict(x[str(normalize_size)])\n",
    "            predictions[model_id] = np.argmax(column_val, axis=1)\n",
    "\n",
    "            four_net_sum += column_val\n",
    "            all_net_sum += column_val\n",
    "\n",
    "        four_net_avg = four_net_sum / 4 # take 4-column average\n",
    "        predictions[str(normalize_size)] = np.argmax(four_net_avg, axis=1)\n",
    "\n",
    "    all_net_avg = all_net_sum / len(models)\n",
    "    predictions['avg'] = np.argmax(all_net_avg, axis=1) # take average of all models\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3RKAqJIhDVxx"
   },
   "outputs": [],
   "source": [
    "def error_rate(prediction, ground_truth):\n",
    "    assert len(prediction) == len(ground_truth)\n",
    "    a = accuracy_score(prediction, ground_truth)\n",
    "    e = 1-a\n",
    "    return f'{e*100:.2f}%' #string format .2f %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cehfNuVKF171"
   },
   "outputs": [],
   "source": [
    "predictions = predict(models, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "Nu-Y0-FH290a",
    "outputId": "bdd15e4f-b48e-4b45-c2b8-e6df4403ef9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29_0 0.72%\n",
      "29_1 0.69%\n",
      "29_2 0.64%\n",
      "29_3 0.59%\n",
      "29 0.48%\n",
      "27_0 0.59%\n",
      "27_1 0.64%\n",
      "27_2 0.64%\n",
      "27_3 0.65%\n",
      "27 0.45%\n",
      "25_0 0.58%\n",
      "25_1 0.58%\n",
      "25_2 0.73%\n",
      "25_3 0.56%\n",
      "25 0.45%\n",
      "21_0 0.66%\n",
      "21_1 0.61%\n",
      "21_2 0.72%\n",
      "21_3 0.71%\n",
      "21 0.48%\n",
      "17_0 0.71%\n",
      "17_1 0.60%\n",
      "17_2 0.59%\n",
      "17_3 0.70%\n",
      "17 0.48%\n",
      "avg 0.40%\n"
     ]
    }
   ],
   "source": [
    "for model_id, pred in predictions.items():\n",
    "    print(model_id, error_rate(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-UB4S3WGr8u"
   },
   "source": [
    "### Weighted ensemble: <br>\n",
    "- Output of the MCDNN: (batch, n_class, n_column, n_channel) = (n, 10, 20, 1)\n",
    "- simple ensembler optimizes weights for each column\n",
    "- more complex ensembler optimizes weights for each unit in columns\n",
    "- the ensembler sum up all columns with their weights and apply softmax function which results in output with shape (n,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4tI13LUOGyv1"
   },
   "outputs": [],
   "source": [
    "# Create data for training weights for each model\n",
    "def get_model_outputs(models, x):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        models: a dict of models that was trained and loaded above\n",
    "        x: a dict of transformed testsets (#n, h, w, channels)\n",
    "    Return:\n",
    "        outputs: 20 columns get stacked together (#n, 10, 20, 1) and a dict maping model_id to column id\n",
    "    \"\"\"\n",
    "    outputs = np.zeros((x['29'].shape[0],10,20,1), dtype='float32')\n",
    "    model_to_column = dict()\n",
    "    \n",
    "    i = 0\n",
    "    for normalize_size in NORMALIZE_SIZES:\n",
    "        for column in range(4):\n",
    "            model_id = f'{normalize_size}_{column}'\n",
    "            column_val = models[model_id].predict(x[str(normalize_size)])\n",
    "            outputs[:,:,i,0] = column_val\n",
    "            model_to_column[model_id] = i\n",
    "            i += 1\n",
    "            \n",
    "    return (outputs, model_to_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10, 20, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns, model_to_columnId = get_model_outputs(models, data_train)\n",
    "columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensembler(x, y):\n",
    "    \"\"\"\n",
    "    x: outputs of 20 models (42000,10,20,1)\n",
    "    y: label (42000,10)\n",
    "    return: weights for models\n",
    "    \"\"\"\n",
    "    # prepare data\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size = 0.2, random_state=42)\n",
    "    \n",
    "    ensembler = Sequential()\n",
    "    ensembler.add(Conv2D(filters=1, kernel_size=(1,20), padding='valid', input_shape=(10,20,1)))\n",
    "    ensembler.add(Flatten())\n",
    "    ensembler.add(Activation('softmax'))\n",
    "    \n",
    "    annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "    ensembler.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "    ensembler.fit(x=x_train, y=y_train, epochs=42, batch_size=256, validation_data=(x_dev, y_dev), callbacks=[annealer], verbose=2)\n",
    "    \n",
    "    return ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/42\n",
      " - 1s - loss: 1.8673 - accuracy: 0.7166 - val_loss: 0.9511 - val_accuracy: 0.9990\n",
      "Epoch 2/42\n",
      " - 1s - loss: 0.5555 - accuracy: 0.9993 - val_loss: 0.3051 - val_accuracy: 0.9993\n",
      "Epoch 3/42\n",
      " - 0s - loss: 0.2133 - accuracy: 0.9993 - val_loss: 0.1477 - val_accuracy: 0.9995\n",
      "Epoch 4/42\n",
      " - 0s - loss: 0.1163 - accuracy: 0.9994 - val_loss: 0.0905 - val_accuracy: 0.9996\n",
      "Epoch 5/42\n",
      " - 1s - loss: 0.0763 - accuracy: 0.9994 - val_loss: 0.0630 - val_accuracy: 0.9996\n",
      "Epoch 6/42\n",
      " - 0s - loss: 0.0554 - accuracy: 0.9994 - val_loss: 0.0473 - val_accuracy: 0.9995\n",
      "Epoch 7/42\n",
      " - 0s - loss: 0.0429 - accuracy: 0.9994 - val_loss: 0.0374 - val_accuracy: 0.9995\n",
      "Epoch 8/42\n",
      " - 0s - loss: 0.0347 - accuracy: 0.9994 - val_loss: 0.0306 - val_accuracy: 0.9995\n",
      "Epoch 9/42\n",
      " - 1s - loss: 0.0290 - accuracy: 0.9994 - val_loss: 0.0258 - val_accuracy: 0.9995\n",
      "Epoch 10/42\n",
      " - 1s - loss: 0.0249 - accuracy: 0.9994 - val_loss: 0.0222 - val_accuracy: 0.9995\n",
      "Epoch 11/42\n",
      " - 1s - loss: 0.0217 - accuracy: 0.9994 - val_loss: 0.0194 - val_accuracy: 0.9995\n",
      "Epoch 12/42\n",
      " - 0s - loss: 0.0193 - accuracy: 0.9994 - val_loss: 0.0172 - val_accuracy: 0.9995\n",
      "Epoch 13/42\n",
      " - 0s - loss: 0.0173 - accuracy: 0.9994 - val_loss: 0.0154 - val_accuracy: 0.9995\n",
      "Epoch 14/42\n",
      " - 1s - loss: 0.0157 - accuracy: 0.9994 - val_loss: 0.0139 - val_accuracy: 0.9995\n",
      "Epoch 15/42\n",
      " - 1s - loss: 0.0144 - accuracy: 0.9994 - val_loss: 0.0127 - val_accuracy: 0.9995\n",
      "Epoch 16/42\n",
      " - 0s - loss: 0.0133 - accuracy: 0.9994 - val_loss: 0.0117 - val_accuracy: 0.9995\n",
      "Epoch 17/42\n",
      " - 0s - loss: 0.0123 - accuracy: 0.9994 - val_loss: 0.0108 - val_accuracy: 0.9996\n",
      "Epoch 18/42\n",
      " - 0s - loss: 0.0115 - accuracy: 0.9994 - val_loss: 0.0100 - val_accuracy: 0.9996\n",
      "Epoch 19/42\n",
      " - 1s - loss: 0.0108 - accuracy: 0.9994 - val_loss: 0.0093 - val_accuracy: 0.9996\n",
      "Epoch 20/42\n",
      " - 0s - loss: 0.0102 - accuracy: 0.9994 - val_loss: 0.0087 - val_accuracy: 0.9996\n",
      "Epoch 21/42\n",
      " - 1s - loss: 0.0097 - accuracy: 0.9994 - val_loss: 0.0082 - val_accuracy: 0.9996\n",
      "Epoch 22/42\n",
      " - 0s - loss: 0.0092 - accuracy: 0.9994 - val_loss: 0.0077 - val_accuracy: 0.9996\n",
      "Epoch 23/42\n",
      " - 0s - loss: 0.0088 - accuracy: 0.9994 - val_loss: 0.0073 - val_accuracy: 0.9996\n",
      "Epoch 24/42\n",
      " - 0s - loss: 0.0084 - accuracy: 0.9994 - val_loss: 0.0070 - val_accuracy: 0.9996\n",
      "Epoch 25/42\n",
      " - 1s - loss: 0.0081 - accuracy: 0.9994 - val_loss: 0.0066 - val_accuracy: 0.9996\n",
      "Epoch 26/42\n",
      " - 0s - loss: 0.0078 - accuracy: 0.9994 - val_loss: 0.0063 - val_accuracy: 0.9996\n",
      "Epoch 27/42\n",
      " - 0s - loss: 0.0075 - accuracy: 0.9994 - val_loss: 0.0060 - val_accuracy: 0.9996\n",
      "Epoch 28/42\n",
      " - 0s - loss: 0.0072 - accuracy: 0.9994 - val_loss: 0.0058 - val_accuracy: 0.9996\n",
      "Epoch 29/42\n",
      " - 0s - loss: 0.0070 - accuracy: 0.9994 - val_loss: 0.0055 - val_accuracy: 0.9996\n",
      "Epoch 30/42\n",
      " - 0s - loss: 0.0068 - accuracy: 0.9994 - val_loss: 0.0053 - val_accuracy: 0.9996\n",
      "Epoch 31/42\n",
      " - 1s - loss: 0.0066 - accuracy: 0.9994 - val_loss: 0.0051 - val_accuracy: 0.9996\n",
      "Epoch 32/42\n",
      " - 0s - loss: 0.0064 - accuracy: 0.9994 - val_loss: 0.0049 - val_accuracy: 0.9996\n",
      "Epoch 33/42\n",
      " - 0s - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.0048 - val_accuracy: 0.9996\n",
      "Epoch 34/42\n",
      " - 0s - loss: 0.0061 - accuracy: 0.9994 - val_loss: 0.0046 - val_accuracy: 0.9996\n",
      "Epoch 35/42\n",
      " - 0s - loss: 0.0059 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9996\n",
      "Epoch 36/42\n",
      " - 1s - loss: 0.0058 - accuracy: 0.9994 - val_loss: 0.0043 - val_accuracy: 0.9996\n",
      "Epoch 37/42\n",
      " - 0s - loss: 0.0057 - accuracy: 0.9994 - val_loss: 0.0042 - val_accuracy: 0.9996\n",
      "Epoch 38/42\n",
      " - 0s - loss: 0.0056 - accuracy: 0.9994 - val_loss: 0.0041 - val_accuracy: 0.9996\n",
      "Epoch 39/42\n",
      " - 1s - loss: 0.0055 - accuracy: 0.9994 - val_loss: 0.0039 - val_accuracy: 0.9996\n",
      "Epoch 40/42\n",
      " - 0s - loss: 0.0053 - accuracy: 0.9994 - val_loss: 0.0038 - val_accuracy: 0.9996\n",
      "Epoch 41/42\n",
      " - 1s - loss: 0.0053 - accuracy: 0.9994 - val_loss: 0.0037 - val_accuracy: 0.9996\n",
      "Epoch 42/42\n",
      " - 0s - loss: 0.0052 - accuracy: 0.9994 - val_loss: 0.0036 - val_accuracy: 0.9996\n"
     ]
    }
   ],
   "source": [
    "ensembler1 = train_ensembler(columns, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[0.5282702 ]],\n",
      "\n",
      "        [[0.34110475]],\n",
      "\n",
      "        [[0.79952043]],\n",
      "\n",
      "        [[0.57873416]],\n",
      "\n",
      "        [[0.6989094 ]],\n",
      "\n",
      "        [[0.32333928]],\n",
      "\n",
      "        [[0.5689375 ]],\n",
      "\n",
      "        [[0.13514563]],\n",
      "\n",
      "        [[0.1143741 ]],\n",
      "\n",
      "        [[0.79800946]],\n",
      "\n",
      "        [[0.47129872]],\n",
      "\n",
      "        [[0.2930282 ]],\n",
      "\n",
      "        [[0.4485501 ]],\n",
      "\n",
      "        [[0.33545163]],\n",
      "\n",
      "        [[0.3575827 ]],\n",
      "\n",
      "        [[0.05459918]],\n",
      "\n",
      "        [[0.25592005]],\n",
      "\n",
      "        [[0.19542356]],\n",
      "\n",
      "        [[0.05737453]],\n",
      "\n",
      "        [[0.8009204 ]]]], dtype=float32), array([0.00027843], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "weights = ensembler1.get_layer(index=0).get_weights()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_complex_ensembler(x, y):\n",
    "    # prepare data\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size = 0.2, random_state=42)\n",
    "    \n",
    "    ensembler = Sequential()\n",
    "    ensembler.add(Conv2D(filters=10, kernel_size=(1,20), padding='valid', input_shape=(10,20,1), activation='relu'))\n",
    "    ensembler.add(Conv2D(filters=1, kernel_size=(1,1), padding='valid'))\n",
    "    ensembler.add(Flatten())\n",
    "    ensembler.add(Activation('softmax'))\n",
    "    \n",
    "    annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "    ensembler.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "    ensembler.fit(x=x_train, y=y_train, epochs=30, batch_size=256, validation_data=(x_dev, y_dev), callbacks=[annealer], verbose=2)\n",
    "    \n",
    "    return ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/30\n",
      " - 1s - loss: 1.0274 - accuracy: 0.9994 - val_loss: 0.2151 - val_accuracy: 0.9999\n",
      "Epoch 2/30\n",
      " - 1s - loss: 0.0939 - accuracy: 0.9994 - val_loss: 0.0398 - val_accuracy: 0.9999\n",
      "Epoch 3/30\n",
      " - 1s - loss: 0.0279 - accuracy: 0.9994 - val_loss: 0.0178 - val_accuracy: 0.9999\n",
      "Epoch 4/30\n",
      " - 1s - loss: 0.0152 - accuracy: 0.9994 - val_loss: 0.0106 - val_accuracy: 0.9999\n",
      "Epoch 5/30\n",
      " - 1s - loss: 0.0103 - accuracy: 0.9994 - val_loss: 0.0073 - val_accuracy: 0.9999\n",
      "Epoch 6/30\n",
      " - 1s - loss: 0.0079 - accuracy: 0.9994 - val_loss: 0.0055 - val_accuracy: 0.9999\n",
      "Epoch 7/30\n",
      " - 1s - loss: 0.0066 - accuracy: 0.9994 - val_loss: 0.0044 - val_accuracy: 0.9999\n",
      "Epoch 8/30\n",
      " - 1s - loss: 0.0048 - accuracy: 0.9994 - val_loss: 0.0014 - val_accuracy: 0.9999\n",
      "Epoch 9/30\n",
      " - 1s - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0010 - val_accuracy: 0.9999\n",
      "Epoch 10/30\n",
      " - 1s - loss: 0.0032 - accuracy: 0.9994 - val_loss: 9.4654e-04 - val_accuracy: 0.9999\n",
      "Epoch 11/30\n",
      " - 1s - loss: 0.0032 - accuracy: 0.9994 - val_loss: 9.1518e-04 - val_accuracy: 0.9999\n",
      "Epoch 12/30\n",
      " - 1s - loss: 0.0031 - accuracy: 0.9994 - val_loss: 9.1305e-04 - val_accuracy: 0.9999\n",
      "Epoch 13/30\n",
      " - 1s - loss: 0.0031 - accuracy: 0.9994 - val_loss: 8.9693e-04 - val_accuracy: 0.9999\n",
      "Epoch 14/30\n",
      " - 1s - loss: 0.0031 - accuracy: 0.9995 - val_loss: 8.8800e-04 - val_accuracy: 0.9999\n",
      "Epoch 15/30\n",
      " - 1s - loss: 0.0031 - accuracy: 0.9995 - val_loss: 8.8209e-04 - val_accuracy: 0.9999\n",
      "Epoch 16/30\n",
      " - 1s - loss: 0.0031 - accuracy: 0.9995 - val_loss: 8.8369e-04 - val_accuracy: 0.9999\n",
      "Epoch 17/30\n",
      " - 1s - loss: 0.0031 - accuracy: 0.9995 - val_loss: 8.8153e-04 - val_accuracy: 0.9999\n",
      "Epoch 18/30\n",
      " - 1s - loss: 0.0031 - accuracy: 0.9995 - val_loss: 8.7637e-04 - val_accuracy: 0.9999\n",
      "Epoch 19/30\n",
      " - 1s - loss: 0.0031 - accuracy: 0.9995 - val_loss: 8.8180e-04 - val_accuracy: 0.9999\n",
      "Epoch 20/30\n",
      " - 1s - loss: 0.0031 - accuracy: 0.9995 - val_loss: 8.7690e-04 - val_accuracy: 0.9999\n",
      "Epoch 21/30\n",
      " - 1s - loss: 0.0031 - accuracy: 0.9995 - val_loss: 8.7097e-04 - val_accuracy: 0.9999\n",
      "Epoch 22/30\n",
      " - 1s - loss: 0.0030 - accuracy: 0.9995 - val_loss: 8.7891e-04 - val_accuracy: 0.9999\n",
      "Epoch 23/30\n",
      " - 1s - loss: 0.0030 - accuracy: 0.9995 - val_loss: 8.7434e-04 - val_accuracy: 0.9999\n",
      "Epoch 24/30\n",
      " - 1s - loss: 0.0030 - accuracy: 0.9995 - val_loss: 8.7401e-04 - val_accuracy: 0.9999\n",
      "Epoch 25/30\n",
      " - 1s - loss: 0.0030 - accuracy: 0.9995 - val_loss: 8.7179e-04 - val_accuracy: 0.9999\n",
      "Epoch 26/30\n",
      " - 1s - loss: 0.0030 - accuracy: 0.9995 - val_loss: 8.7082e-04 - val_accuracy: 0.9999\n",
      "Epoch 27/30\n",
      " - 1s - loss: 0.0030 - accuracy: 0.9995 - val_loss: 8.6943e-04 - val_accuracy: 0.9999\n",
      "Epoch 28/30\n",
      " - 1s - loss: 0.0030 - accuracy: 0.9995 - val_loss: 8.6673e-04 - val_accuracy: 0.9999\n",
      "Epoch 29/30\n",
      " - 1s - loss: 0.0030 - accuracy: 0.9995 - val_loss: 8.6867e-04 - val_accuracy: 0.9999\n",
      "Epoch 30/30\n",
      " - 1s - loss: 0.0030 - accuracy: 0.9995 - val_loss: 8.7163e-04 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "ensembler2 = train_complex_ensembler(columns, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 10, 1, 10)         210       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 10, 1, 1)          11        \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ensembler2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ensembler on testset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_columns, _ = get_model_outputs(models, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.39%'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = ensembler1.predict(testset_columns)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "error_rate(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.38%'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = ensembler2.predict(testset_columns)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "error_rate(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MCDNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
